{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before we proceed, let's check that we're using the right image, that is, TensorFlow is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip3 list | grep tensorflow \n",
    "#! pip3 install --user tensorflow==2.4.0\n",
    "#! pip3 install --user ipywidgets nbconvert\n",
    "#!python -m pip install --user --upgrade pip\n",
    "#!pip3 install pandas scikit-learn keras tensorflow-datasets --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To package the trainer in a container image, we shall need a file (on our cluster) that contains the code as well as a file with the resource definitition of the job for the Kubernetes cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER_FILE = \"tfjobheart.py\"\n",
    "KUBERNETES_FILE = \"tfjob-heartdisease.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We also want to capture output from a cell with %%capture that usually looks like some-resource created. To that end, let's define a helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from IPython.utils.capture import CapturedIO\n",
    "\n",
    "\n",
    "def get_resource(captured_io: CapturedIO) -> str:\n",
    "    \"\"\"\n",
    "    Gets a resource name from `kubectl apply -f <configuration.yaml>`.\n",
    "\n",
    "    :param str captured_io: Output captured by using `%%capture` cell magic\n",
    "    :return: Name of the Kubernetes resource\n",
    "    :rtype: str\n",
    "    :raises Exception: if the resource could not be created\n",
    "    \"\"\"\n",
    "    out = captured_io.stdout\n",
    "    matches = re.search(r\"^(.+)\\s+created\", out)\n",
    "    if matches is not None:\n",
    "        return matches.group(1)\n",
    "    else:\n",
    "        raise Exception(f\"Cannot get resource as its creation failed: {out}. It may already exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Inspect the DataÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trtbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalachh</th>\n",
       "      <th>exng</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slp</th>\n",
       "      <th>caa</th>\n",
       "      <th>thall</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trtbps  chol  fbs  restecg  thalachh  exng  oldpeak  slp  \\\n",
       "0   63    1   3     145   233    1        0       150     0      2.3    0   \n",
       "1   37    1   2     130   250    0        1       187     0      3.5    0   \n",
       "2   41    0   1     130   204    0        0       172     0      1.4    2   \n",
       "3   56    1   1     120   236    0        1       178     0      0.8    2   \n",
       "4   57    0   0     120   354    0        1       163     1      0.6    2   \n",
       "\n",
       "   caa  thall  output  \n",
       "0    0      1       1  \n",
       "1    0      2       1  \n",
       "2    0      2       1  \n",
       "3    0      2       1  \n",
       "4    0      2       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as  pd\n",
    "data = pd.read_csv(\"heart.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model in the Notebook\n",
    "\n",
    "We want to train the model in a distributed fashion, we put all the code in a single cell. That way we can save the file and include it in a container image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tfjobheart.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $TRAINER_FILE\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from numpy.random import seed\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(221)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "def make_datasets_unbatched():\n",
    "    data = pd.read_csv(\"heart.csv\")\n",
    "    data.head()\n",
    "    \n",
    "    data.apply(lambda x: sum(x.isnull()),axis=0)\n",
    "    \n",
    "    # List of variables with missing values\n",
    "    \n",
    "    vars_with_na=[var for var in data.columns if data[var].isnull().sum()>1]\n",
    "    \n",
    "    #Boolan variables\n",
    "    bool_var=['sex','output','fbs', 'exng']\n",
    "    #Categorical variables:cardinalty\n",
    "    cat_var=['cp','restecg','slp', 'thall']\n",
    "    #discrete variables\n",
    "    num_var=['age', 'trtbps','chol', 'thalachh', 'oldpeak', 'caa']\n",
    "    #remove outliers\n",
    "    \n",
    "    def removeOutlier(att, data):\n",
    "        lowerbound = att.mean() - 3 * att.std()\n",
    "        upperbound = att.mean() + 3 * att.std()\n",
    "        #print('lowerbound: ',lowerbound,' -------- upperbound: ', upperbound )\n",
    "        df1 = data[(att > lowerbound) & (att < upperbound)]\n",
    "        #print((data.shape[0] - df1.shape[0]), ' number of outliers from ', data.shape[0] )\n",
    "        #print(' ******************************************************')\n",
    "        data = df1.copy()\n",
    "        return data\n",
    "    data = removeOutlier(data.trtbps, data)\n",
    "    data = removeOutlier(data.chol, data)\n",
    "    \n",
    "    #resampling\n",
    "    from sklearn.utils import resample\n",
    "    \n",
    "    # Separate Target Classes\n",
    "    df_1 = data[data.output==1]\n",
    "    df_2 = data[data.output==0]\n",
    "    \n",
    "    # Upsample minority class\n",
    "    df_upsample_1 = resample(df_2, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=163,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    "    # Combine majority class with upsampled minority class\n",
    "    df_upsampled = pd.concat([df_1, df_upsample_1])\n",
    "    \n",
    "    # Display new class counts\n",
    "    df_upsampled.output.value_counts()\n",
    "    \n",
    "    x = df_upsampled.drop('output', axis = 1)\n",
    "    y = df_upsampled['output']\n",
    "    \n",
    "    #Split dataset\n",
    "\n",
    "    \n",
    "    x_train,x_test, y_train, y_test = tts(x,y, test_size = 0.2, random_state = 111)\n",
    "    \n",
    "    #Scaling\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.fit_transform(x_test)\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    train = train_dataset.cache().shuffle(2000).repeat()\n",
    "    return train, test_dataset\n",
    "\n",
    "def model(args):\n",
    "    seed(1)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_dim=13))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.summary()\n",
    "    opt = args.optimizer\n",
    "    model.compile(optimizer=opt,\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    tf.keras.backend.set_value(model.optimizer.learning_rate, args.learning_rate)\n",
    "    return model\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # MultiWorkerMirroredStrategy creates copies of all variables in the model's\n",
    "    # layers on each device across all workers\n",
    "    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(\n",
    "        communication=tf.distribute.experimental.CollectiveCommunication.AUTO)\n",
    "    logging.debug(f\"num_replicas_in_sync: {strategy.num_replicas_in_sync}\")\n",
    "    BATCH_SIZE_PER_REPLICA = args.batch_size\n",
    "    BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "    \n",
    "    # Datasets need to be created after instantiation of `MultiWorkerMirroredStrategy`\n",
    "    train_dataset, test_dataset = make_datasets_unbatched()\n",
    "    train_dataset = train_dataset.batch(batch_size=BATCH_SIZE)\n",
    "    test_dataset = test_dataset.batch(batch_size=BATCH_SIZE)\n",
    "\n",
    "    # See: https://www.tensorflow.org/api_docs/python/tf/data/experimental/DistributeOptions\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = \\\n",
    "    tf.data.experimental.AutoShardPolicy.DATA\n",
    "    \n",
    "    train_datasets_sharded  = train_dataset.with_options(options)\n",
    "    test_dataset_sharded = test_dataset.with_options(options)\n",
    "    \n",
    "    with strategy.scope():\n",
    "        # Model building/compiling need to be within `strategy.scope()`.\n",
    "        multi_worker_model = model(args)\n",
    "        # Keras' `model.fit()` trains the model with specified number of epochs and\n",
    "        # number of steps per epoch. \n",
    "        multi_worker_model.fit(train_datasets_sharded,\n",
    "                         epochs=50,\n",
    "                         steps_per_epoch=30)\n",
    "\n",
    "        eval_loss, eval_acc = multi_worker_model.evaluate(test_dataset_sharded, \n",
    "                                                    verbose=0, steps=10)\n",
    "        # Log metrics for Katib\n",
    "        logging.info(\"loss={:.4f}\".format(eval_loss))\n",
    "        logging.info(\"accuracy={:.4f}\".format(eval_acc))\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--batch_size\",\n",
    "                      type=int,\n",
    "                      default=32,\n",
    "                      metavar=\"N\",\n",
    "                      help=\"Batch size for training (default: 128)\")\n",
    "    parser.add_argument(\"--learning_rate\", \n",
    "                      type=float,  \n",
    "                      default=0.1,\n",
    "                      metavar=\"N\",\n",
    "                      help='Initial learning rate')\n",
    "    parser.add_argument(\"--optimizer\", \n",
    "                      type=str, \n",
    "                      default='adam',\n",
    "                      metavar=\"N\",\n",
    "                      help='optimizer')\n",
    "    parsed_args, _ = parser.parse_known_args()\n",
    "    main(parsed_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### That saves the file as defined by TRAINER_FILE but it does not run it.\n",
    "\n",
    "Let's see if our code is correct by running it from within our notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 10)                140       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 261\n",
      "Trainable params: 261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "30/30 [==============================] - 3s 3ms/step - loss: 0.4935 - accuracy: 0.7704\n",
      "Epoch 2/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.2807 - accuracy: 0.8891\n",
      "Epoch 3/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.2270 - accuracy: 0.9010\n",
      "Epoch 4/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.1995 - accuracy: 0.9093\n",
      "Epoch 5/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.1502 - accuracy: 0.9280\n",
      "Epoch 6/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.1591 - accuracy: 0.9293\n",
      "Epoch 7/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.0997 - accuracy: 0.9499\n",
      "Epoch 8/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.1808 - accuracy: 0.9358\n",
      "Epoch 9/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.1182 - accuracy: 0.9511\n",
      "Epoch 10/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.0696 - accuracy: 0.9635\n",
      "Epoch 11/50\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0702 - accuracy: 0.9649\n",
      "Epoch 12/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.0585 - accuracy: 0.9701\n",
      "Epoch 13/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.1000 - accuracy: 0.9579\n",
      "Epoch 14/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.1452 - accuracy: 0.9369\n",
      "Epoch 15/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.2288 - accuracy: 0.9217\n",
      "Epoch 16/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.1884 - accuracy: 0.9119\n",
      "Epoch 17/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.0874 - accuracy: 0.9584\n",
      "Epoch 18/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.0651 - accuracy: 0.9678\n",
      "Epoch 19/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9815\n",
      "Epoch 20/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.0735 - accuracy: 0.9692\n",
      "Epoch 21/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.1589 - accuracy: 0.9555\n",
      "Epoch 22/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.1197 - accuracy: 0.9449\n",
      "Epoch 23/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.0777 - accuracy: 0.9731\n",
      "Epoch 24/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.0664 - accuracy: 0.9772\n",
      "Epoch 25/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.0621 - accuracy: 0.9750\n",
      "Epoch 26/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.0635 - accuracy: 0.9727\n",
      "Epoch 27/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.0484 - accuracy: 0.9791\n",
      "Epoch 28/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.0550 - accuracy: 0.9752\n",
      "Epoch 29/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.0543 - accuracy: 0.9739\n",
      "Epoch 30/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.0512 - accuracy: 0.9770\n",
      "Epoch 31/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.0460 - accuracy: 0.9788\n",
      "Epoch 32/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.0419 - accuracy: 0.9804\n",
      "Epoch 33/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.0440 - accuracy: 0.9811\n",
      "Epoch 34/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0608 - accuracy: 0.9697\n",
      "Epoch 35/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.0432 - accuracy: 0.9820\n",
      "Epoch 36/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.0377 - accuracy: 0.9852\n",
      "Epoch 37/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9780\n",
      "Epoch 38/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.0389 - accuracy: 0.9834\n",
      "Epoch 39/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0561 - accuracy: 0.9721\n",
      "Epoch 40/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9750\n",
      "Epoch 41/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.0413 - accuracy: 0.9828\n",
      "Epoch 42/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9757\n",
      "Epoch 43/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.0483 - accuracy: 0.9787\n",
      "Epoch 44/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0522 - accuracy: 0.9761\n",
      "Epoch 45/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.0455 - accuracy: 0.9792\n",
      "Epoch 46/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.0559 - accuracy: 0.9749\n",
      "Epoch 47/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.0446 - accuracy: 0.9815\n",
      "Epoch 48/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 0.9768\n",
      "Epoch 49/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0541 - accuracy: 0.9738\n",
      "Epoch 50/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.0459 - accuracy: 0.9821\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10 batches). You may need to use the repeat() function when building your dataset.\n",
      "INFO:root:loss=1.2972\n",
      "INFO:root:accuracy=0.9091\n"
     ]
    }
   ],
   "source": [
    "%run $TRAINER_FILE --optimizer 'adam'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Distributed TFJob\n",
    "\n",
    "\n",
    "For large training jobs, we wish to run our trainer in a distributed mode. Once the notebook server cluster can access the Docker image from the registry, we can launch a distributed TF job.\n",
    "\n",
    "The specification for a distributed TFJob is defined using YAML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tfjob-heartdisease.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $KUBERNETES_FILE\n",
    "apiVersion: \"kubeflow.org/v1\"\n",
    "kind: \"TFJob\"\n",
    "metadata:\n",
    "  name: \"hrtd\"\n",
    "  namespace: ekemini # your-user-namespace\n",
    "spec:\n",
    "  cleanPodPolicy: None\n",
    "  tfReplicaSpecs:\n",
    "    Worker:\n",
    "      replicas: 2\n",
    "      restartPolicy: OnFailure\n",
    "      template:\n",
    "        metadata:\n",
    "          annotations:\n",
    "            sidecar.istio.io/inject: \"false\"\n",
    "        spec:\n",
    "          containers:\n",
    "          - name: tensorflow\n",
    "            # modify this property if you would like to use a custom image\n",
    "            image: mavencodevv/tfjob_heart:v.0.1\n",
    "            command:\n",
    "                - \"python\"\n",
    "                - \"/tfjobheart.py\"\n",
    "                - \"--batch_size=64\"\n",
    "                - \"--learning_rate=0.1\"\n",
    "                - \"--optimizer=adam\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's deploy the distributed training job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture tf_output --no-stderr\n",
    "! kubectl create -f $KUBERNETES_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_JOB = get_resource(tf_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To see the job status, use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:         hrtd\n",
      "Namespace:    ekemini\n",
      "Labels:       <none>\n",
      "Annotations:  <none>\n",
      "API Version:  kubeflow.org/v1\n",
      "Kind:         TFJob\n",
      "Metadata:\n",
      "  Creation Timestamp:  2021-04-13T05:59:54Z\n",
      "  Generation:          1\n",
      "  Managed Fields:\n",
      "    API Version:  kubeflow.org/v1\n",
      "    Fields Type:  FieldsV1\n",
      "    fieldsV1:\n",
      "      f:spec:\n",
      "        .:\n",
      "        f:cleanPodPolicy:\n",
      "        f:tfReplicaSpecs:\n",
      "          .:\n",
      "          f:Worker:\n",
      "            .:\n",
      "            f:replicas:\n",
      "            f:restartPolicy:\n",
      "            f:template:\n",
      "              .:\n",
      "              f:metadata:\n",
      "                .:\n",
      "                f:annotations:\n",
      "                  .:\n",
      "                  f:sidecar.istio.io/inject:\n",
      "              f:spec:\n",
      "    Manager:      kubectl-create\n",
      "    Operation:    Update\n",
      "    Time:         2021-04-13T05:59:54Z\n",
      "    API Version:  kubeflow.org/v1\n",
      "    Fields Type:  FieldsV1\n",
      "    fieldsV1:\n",
      "      f:spec:\n",
      "        f:successPolicy:\n",
      "        f:tfReplicaSpecs:\n",
      "          f:Worker:\n",
      "            f:template:\n",
      "              f:metadata:\n",
      "                f:creationTimestamp:\n",
      "              f:spec:\n",
      "                f:containers:\n",
      "      f:status:\n",
      "        .:\n",
      "        f:conditions:\n",
      "        f:replicaStatuses:\n",
      "          .:\n",
      "          f:Worker:\n",
      "        f:startTime:\n",
      "    Manager:         tf-operator.v1\n",
      "    Operation:       Update\n",
      "    Time:            2021-04-13T05:59:54Z\n",
      "  Resource Version:  10221608\n",
      "  Self Link:         /apis/kubeflow.org/v1/namespaces/ekemini/tfjobs/hrtd\n",
      "  UID:               d7a9531c-5b2f-40e3-bbe2-8f245a878c19\n",
      "Spec:\n",
      "  Clean Pod Policy:  None\n",
      "  Tf Replica Specs:\n",
      "    Worker:\n",
      "      Replicas:        2\n",
      "      Restart Policy:  OnFailure\n",
      "      Template:\n",
      "        Metadata:\n",
      "          Annotations:\n",
      "            sidecar.istio.io/inject:  false\n",
      "        Spec:\n",
      "          Containers:\n",
      "            Command:\n",
      "              python\n",
      "              /tfjobheart.py\n",
      "              --batch_size=64\n",
      "              --learning_rate=0.1\n",
      "              --optimizer=adam\n",
      "            Image:  mavencodevv/tfjob_heart:v.0.1\n",
      "            Name:   tensorflow\n",
      "Status:\n",
      "  Conditions:\n",
      "    Last Transition Time:  2021-04-13T05:59:54Z\n",
      "    Last Update Time:      2021-04-13T05:59:54Z\n",
      "    Message:               TFJob hrtd is created.\n",
      "    Reason:                TFJobCreated\n",
      "    Status:                True\n",
      "    Type:                  Created\n",
      "  Replica Statuses:\n",
      "    Worker:\n",
      "  Start Time:  2021-04-13T05:59:54Z\n",
      "Events:\n",
      "  Type    Reason                   Age   From         Message\n",
      "  ----    ------                   ----  ----         -------\n",
      "  Normal  SuccessfulCreatePod      1s    tf-operator  Created pod: hrtd-worker-0\n",
      "  Normal  SuccessfulCreatePod      1s    tf-operator  Created pod: hrtd-worker-1\n",
      "  Normal  SuccessfulCreateService  1s    tf-operator  Created service: hrtd-worker-0\n",
      "  Normal  SuccessfulCreateService  1s    tf-operator  Created service: hrtd-worker-1\n"
     ]
    }
   ],
   "source": [
    "! kubectl describe $TF_JOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME            READY   STATUS              RESTARTS   AGE\n",
      "hrtd-worker-0   0/1     ContainerCreating   0          2s\n",
      "hrtd-worker-1   0/1     ContainerCreating   0          2s\n"
     ]
    }
   ],
   "source": [
    "! kubectl get pods -l job-name=hrtd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3s          Normal    Scheduled                 pod/hrtd-worker-1                                                          Successfully assigned ekemini/hrtd-worker-1 to ip-10-0-19-4.ec2.internal\n",
      "3s          Normal    Scheduled                 pod/hrtd-worker-0                                                          Successfully assigned ekemini/hrtd-worker-0 to ip-10-0-10-33.ec2.internal\n",
      "3s          Normal    SuccessfulCreatePod       tfjob/hrtd                                                                 Created pod: hrtd-worker-1\n",
      "3s          Normal    SuccessfulCreateService   tfjob/hrtd                                                                 Created service: hrtd-worker-0\n",
      "2s          Normal    Pulled                    pod/hrtd-worker-1                                                          Container image \"mavencodevv/tfjob_heart:v.0.1\" already present on machine\n",
      "2s          Normal    Started                   pod/hrtd-worker-0                                                          Started container tensorflow\n",
      "2s          Normal    Pulled                    pod/hrtd-worker-0                                                          Container image \"mavencodevv/tfjob_heart:v.0.1\" already present on machine\n",
      "2s          Normal    Created                   pod/hrtd-worker-0                                                          Created container tensorflow\n",
      "2s          Normal    Created                   pod/hrtd-worker-1                                                          Created container tensorflow\n",
      "2s          Normal    Started                   pod/hrtd-worker-1                                                          Started container tensorflow\n"
     ]
    }
   ],
   "source": [
    "! kubectl get events --sort-by='.lastTimestamp' | tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stream logs from the worker-0 pod to check the training progress, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-13 05:59:56.534068: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-04-13 05:59:56.534103: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "WARNING:tensorflow:From /tfjobheart.py:116: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "use distribute.MultiWorkerMirroredStrategy instead\n",
      "2021-04-13 05:59:58.336024: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-04-13 05:59:58.336264: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-04-13 05:59:58.336288: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-04-13 05:59:58.336313: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (hrtd-worker-0): /proc/driver/nvidia/version does not exist\n",
      "2021-04-13 05:59:58.337647: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-04-13 05:59:58.338653: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-04-13 05:59:58.345910: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> hrtd-worker-0.ekemini.svc:2222, 1 -> hrtd-worker-1.ekemini.svc:2222}\n",
      "2021-04-13 05:59:58.346602: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://hrtd-worker-0.ekemini.svc:2222\n",
      "INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0']\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:worker/task:0',)\n",
      "INFO:tensorflow:Waiting for the cluster, timeout = inf\n",
      "INFO:tensorflow:Cluster is ready.\n",
      "INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['hrtd-worker-0.ekemini.svc:2222', 'hrtd-worker-1.ekemini.svc:2222']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0',), communication = CommunicationImplementation.AUTO\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "2021-04-13 05:59:58.921544: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-04-13 05:59:58.921914: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2300055000 Hz\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 8 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 8 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 8 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 8 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               1400      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 40)                4040      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 5,881\n",
      "Trainable params: 5,681\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 3s 10ms/step - loss: 0.9397 - accuracy: 0.6811\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4664 - accuracy: 0.8078\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3137 - accuracy: 0.8440\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2866 - accuracy: 0.8601\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.2346 - accuracy: 0.8943\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1563 - accuracy: 0.9559\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1580 - accuracy: 0.9292\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1019 - accuracy: 0.9584\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1256 - accuracy: 0.9433\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1299 - accuracy: 0.9448\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10 batches). You may need to use the repeat() function when building your dataset.\n",
      "INFO:root:loss=0.6354\n",
      "INFO:root:accuracy=0.9091\n"
     ]
    }
   ],
   "source": [
    "! kubectl logs -f hrtd-worker-0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To delete the job, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! kubectl delete $TF_JOB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check to see if the check to see if the pod is still up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! kubectl -n ekemini logs -f hrtd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
