{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
    "          verbose=0, warm_start=False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER_FILE = \"lrjobheart.py\"\n",
    "KUBERNETES_FILE = \"lrjob-heartdisease.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from IPython.utils.capture import CapturedIO\n",
    "\n",
    "\n",
    "def get_resource(captured_io: CapturedIO) -> str:\n",
    "    \"\"\"\n",
    "    Gets a resource name from `kubectl apply -f <configuration.yaml>`.\n",
    "\n",
    "    :param str captured_io: Output captured by using `%%capture` cell magic\n",
    "    :return: Name of the Kubernetes resource\n",
    "    :rtype: str\n",
    "    :raises Exception: if the resource could not be created\n",
    "    \"\"\"\n",
    "    out = captured_io.stdout\n",
    "    matches = re.search(r\"^(.+)\\s+created\", out)\n",
    "    if matches is not None:\n",
    "        return matches.group(1)\n",
    "    else:\n",
    "        raise Exception(f\"Cannot get resource as its creation failed: {out}. It may already exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lrjobheart.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $TRAINER_FILE\n",
    "import argparse\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "import pandas as pd\n",
    "\n",
    "#from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from timeit import default_timer as timestamp\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--max_iter',\n",
    "                        type = int,\n",
    "                        default = 100,\n",
    "                        help = 'The number of iterations for solvers to converge')\n",
    "    parser.add_argument('--penalty',\n",
    "                        type = str,\n",
    "                        default = 'l2',\n",
    "                        help = 'The norm used in penalization.')\n",
    "    parser.add_argument('--solver',\n",
    "                        type = str,\n",
    "                        default = 'liblinear',\n",
    "                        help = 'Algorithm for optimization')\n",
    "    args = parser.parse_args()\n",
    "    #args = parser.parse_known_args()\n",
    "\n",
    "\n",
    "    df=pd.read_csv('https://raw.githubusercontent.com/Soot3/testing/master/heart.csv')\n",
    "\n",
    "    def removeOutlier(att, df):\n",
    "\n",
    "      lowerbound = att.mean() - 3 * att.std()\n",
    "      upperbound = att.mean() + 3 * att.std()\n",
    "\n",
    "      df1 = df[(att > lowerbound) & (att < upperbound)]\n",
    "\n",
    "      df = df1.copy()\n",
    "\n",
    "      return df\n",
    "    df = removeOutlier(df.trtbps, df)\n",
    "    df = removeOutlier(df.chol, df)   \n",
    "\n",
    "    # Separate Target Classes\n",
    "    df_1 = df[df.output==1]\n",
    "    df_2 = df[df.output==0]\n",
    "\n",
    "    # Upsample minority class\n",
    "    df_upsample_1 = resample(df_2, \n",
    "                                  replace=True,     # sample with replacement\n",
    "                                  n_samples=163,    # to match majority class\n",
    "                                  random_state=123) # reproducible results\n",
    "\n",
    "    # Combine majority class with upsampled minority class\n",
    "    df_upsampled = pd.concat([df_1, df_upsample_1])\n",
    "    x = df_upsampled.drop('output', axis = 1)\n",
    "    y = df_upsampled['output']  \n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 111)\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.fit_transform(x_test)\n",
    "\n",
    "\n",
    "\n",
    "    params = {\n",
    "        'max_iter': args.max_iter,\n",
    "        'penalty': args.penalty,\n",
    "        'solver': args.solver\n",
    "    }\n",
    "\n",
    "\n",
    "    start = timestamp()\n",
    "    model = lr()\n",
    "    model.fit(x_train, y_train)\n",
    "    stop = timestamp()\n",
    "\n",
    "    print('time=%.3f' % (stop - start))\n",
    "\n",
    "    predictions = model.predict(x_test)\n",
    "\n",
    "    print('accuracy=%.3f' % accuracy_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time=0.002\n",
      "accuracy=0.955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%run $TRAINER_FILE --solver 'liblinear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
