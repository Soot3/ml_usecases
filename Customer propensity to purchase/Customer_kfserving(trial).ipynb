{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Customer_kfserving(trial).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSfrjEEtxKAX"
      },
      "source": [
        "# Kubeflow pipelines: From Training to Serving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3nIH46nxfFO"
      },
      "source": [
        "## Prequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoV5Q7nIxnlu"
      },
      "source": [
        "check to see if kfp is installed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1Fuk9spxJS7"
      },
      "source": [
        "! pip3 show kfp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1DW40t7xd3s"
      },
      "source": [
        "## Configure Credentials\n",
        "In order for KFServing to access MiniO, the credentials must be added to the default account"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRzsA8FkyP52"
      },
      "source": [
        "%%writefile minio_secret.yaml\n",
        "apiVersion: v1\n",
        "kind: Secret\n",
        "metadata:\n",
        "  name: minio-s3-secret\n",
        "  annotations:\n",
        "     serving.kubeflow.org/s3-endpoint: minio-service.kubeflow:9000\n",
        "     serving.kubeflow.org/s3-usehttps: \"0\" # Default: 1. Must be 0 when testing with MinIO!\n",
        "type: Opaque\n",
        "data:\n",
        "  AWS_ACCESS_KEY_ID: bWluaW8=\n",
        "  AWS_SECRET_ACCESS_KEY: bWluaW8xMjM=\n",
        "---\n",
        "apiVersion: v1\n",
        "kind: ServiceAccount\n",
        "metadata:\n",
        "  name: default\n",
        "secrets:\n",
        "  - name: minio-s3-secret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfP9Uc0WyUUm"
      },
      "source": [
        "! kubectl apply -f minio_secret.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwJv_w0Hytia"
      },
      "source": [
        "## Configure access MinIO\n",
        "### Upload the Dataset to MinIO\n",
        "First, we configure credentials for mc, the MinIO command line client. We then use it to create a bucket, upload the dataset to it, and set access policy so that the pipeline can download it from MinIO."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnx9hyaWyh7r"
      },
      "source": [
        "! wget https://dl.min.io/client/mc/release/linux-amd64/mc\n",
        "! chmod +x mc\n",
        "! ./mc --help"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAPfb2JTy8KT"
      },
      "source": [
        "### a. Connect to the MinIO Server"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E23aFBlZy2fV"
      },
      "source": [
        "! ./mc alias set minio http://minio-service.kubeflow:9000 minio minio123"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2fC_w40zFrN"
      },
      "source": [
        "### b. Create a bucket to store the data and export the model to MinIO\n",
        "The bucket is cleared once we are done running the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A9eGgr1zfBy"
      },
      "source": [
        "! ./mc mb minio/customer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSJPA147zj0p"
      },
      "source": [
        "### c. Upload the dataset to the bucket in MinIO\n",
        "The dataset must be in a folder before uploading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJe20242z6g3"
      },
      "source": [
        "! tar --dereference -czf datasets.tar.gz ./datasets\n",
        "! ./mc cp datasets.tar.gz minio/customer/datasets.tar.gz\n",
        "! ./mc policy set download minio/customer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWnmJM9P0JpQ"
      },
      "source": [
        "If the dataset has been downloaded too many times while testing, the following code below can be used to clear out the bucket"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxDGeu1E0U74"
      },
      "source": [
        "# ! ./mc rm --recursive --force minio/mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXq4Vxwz0ZP1"
      },
      "source": [
        "## MinIO Server URL and Credentials"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnCdMh4e0f4z"
      },
      "source": [
        "MINIO_SERVER='minio-service.kubeflow:9000'\n",
        "MINIO_ACCESS_KEY='minio'\n",
        "MINIO_SECRET_KEY='minio123'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OryAF0JB0kKW"
      },
      "source": [
        "## Implement Kubeflow Pipelines Components\n",
        "In this pipeline, we have the following components:\n",
        "*   Customer dataset download component\n",
        "*   Train the Scikit-Learn model\n",
        "*   Evaluate the trained model\n",
        "*   Export the trained model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q43CxG_j1Sdg"
      },
      "source": [
        "from typing import NamedTuple\n",
        "import kfp\n",
        "import kfp.components as components\n",
        "import kfp.dsl as dsl\n",
        "from kfp.components import InputPath, OutputPath #helps define the input & output between the components\n",
        "import kubeflow.fairing.utils\n",
        "NAMESPACE = kubeflow.fairing.utils.get_current_k8s_namespace()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFEqNCtY1aAZ"
      },
      "source": [
        "## Component 1: Download the Customer Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LtNB3Iy1e4b"
      },
      "source": [
        "def download_dataset(minio_server: str, data_dir: OutputPath(str)):\n",
        "    \"\"\"Download the Customer data set to the KFP volume to share it among all steps\"\"\"\n",
        "    import urllib.request\n",
        "    import tarfile\n",
        "    import os\n",
        "    import subprocess\n",
        "\n",
        "    if not os.path.exists(data_dir):\n",
        "        os.makedirs(data_dir)\n",
        "        \n",
        "    #this url leads to your bucket\n",
        "    url = f'http://{minio_server}/customer/datasets.tar.gz'\n",
        "    stream = urllib.request.urlopen(url)\n",
        "    tar = tarfile.open(fileobj=stream, mode=\"r|gz\")\n",
        "    tar.extractall(path=data_dir)\n",
        "    \n",
        "    subprocess.call([\"ls\", \"-lha\", data_dir])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2giOvK121prw"
      },
      "source": [
        "## Component 2: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJJfvYkM5PRs"
      },
      "source": [
        "def train_model(data_dir: InputPath(str), model_dir: OutputPath(str)):\n",
        "  import pickle\n",
        "  import numpy as np\n",
        "  import pandas as pd\n",
        "  from sklearn import preprocessing\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  from imblearn.over_sampling import RandomOverSampler\n",
        "  from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "  # load the dataset\n",
        "  df1 = pd.read_csv('/kaggle/input/customer-propensity-to-purchase-data/training_sample.csv')\n",
        "  df2 = pd.read_csv('/kaggle/input/customer-propensity-to-purchase-data/testing_sample.csv')\n",
        "  df = pd.concat([df1, df2])\n",
        "  df.head()\n",
        "\n",
        "  # drop extraneous features\n",
        "  X = df.drop(['UserID', 'device_mobile', 'ordered', 'sign_in', ], axis=1)\n",
        "  y = df['ordered']\n",
        "\n",
        "  # split to training and test set\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)\n",
        "\n",
        "  # oversample the smallest class of the train set\n",
        "  over_sampler = RandomOverSampler(random_state=42)\n",
        "  X_train, y_train = over_sampler.fit_resample(X_train, y_train)\n",
        "  \n",
        "  # train the model\n",
        "  model = LogisticRegression()\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  # save the train_data as a pickle file to be used by the train component\n",
        "  with open(f'{data_path}/train_data', 'wb') as f:\n",
        "    pickle.dump((X_train,  y_train), f)\n",
        "\n",
        "  # save the test_data as a pickle file to be used by the train component\n",
        "  with open(f'{data_path}/test_data', 'wb') as f:\n",
        "    pickle.dump((X_test,  y_test), f)\n",
        "\n",
        "  # save the model\n",
        "  model.save(f'{data_path}/{model_dir}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMJQFNc1mt8V"
      },
      "source": [
        "## Component 3: Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOZv4uB9mw_X"
      },
      "source": [
        "def evaluate_model(data_dir: InputPath(str), model_dir: InputPath(str), metrics_path: OutputPath(str)\n",
        ") -> NamedTuple(\"EvaluationOutput\", [(\"mlpipeline_metrics\", \"Metrics\")]):\n",
        "  \n",
        "  import json\n",
        "  from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "  # load the dataset\n",
        "  with open(f'{data_path}/{test_data}', 'rb') as f:\n",
        "    test_data = pickle.load(f)\n",
        "\n",
        "  # separate the X_test from the y_test\n",
        "  X_test, y_test = test_data\n",
        "\n",
        "  # load the model\n",
        "  model = load_model(f'{data_path}/{model_dir}')\n",
        "\n",
        "  # use the model to predict on the test set\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  # evaluate the model and print the results \n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "  metrics = {\n",
        "        \"metrics\": [{\"name\": \"accuracy\", \"numberValue\": str(accuracy), \"format\": \"PERCENTAGE\"}]\n",
        "    }\n",
        "\n",
        "  # save the metrics\n",
        "  with open(metrics_path, \"w\") as f:\n",
        "    json.dump(metrics, f)\n",
        "\n",
        "  out_tuple = namedtuple(\"EvaluationOutput\", [\"mlpipeline_metrics\"])\n",
        "\n",
        "  return out_tuple(json.dumps(metrics))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4xnUQIv0hJk"
      },
      "source": [
        "## Component 4: Export the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8UUqnjE0joO"
      },
      "source": [
        "def export_model(\n",
        "    model_dir: InputPath(str),\n",
        "    metrics: InputPath(str),\n",
        "    export_bucket: str,\n",
        "    model_name: str,\n",
        "    model_version: int,\n",
        "    minio_server: str,\n",
        "    minio_access_key: str,\n",
        "    minio_secret_key: str,\n",
        "):\n",
        "    import os\n",
        "    import boto3\n",
        "    from botocore.client import Config\n",
        "    \n",
        "\n",
        "    s3 = boto3.client(\n",
        "        \"s3\",\n",
        "        endpoint_url=f'http://{minio_server}',\n",
        "        aws_access_key_id=minio_access_key,\n",
        "        aws_secret_access_key=minio_secret_key,\n",
        "        config=Config(signature_version=\"s3v4\"),\n",
        "    )\n",
        "\n",
        "    # Create export bucket if it does not yet exist\n",
        "    response = s3.list_buckets()\n",
        "    export_bucket_exists = False\n",
        "\n",
        "    print(response , export_bucket)\n",
        "    for bucket in response[\"Buckets\"]:\n",
        "        if bucket[\"Name\"] == export_bucket:\n",
        "            export_bucket_exists = True\n",
        "\n",
        "    if not export_bucket_exists:\n",
        "        s3.create_bucket(ACL=\"public-read-write\", Bucket=export_bucket)\n",
        "\n",
        "    # Save model files to S3\n",
        "    for root, dirs, files in os.walk(model_dir):\n",
        "        for filename in files:\n",
        "            local_path = os.path.join(root, filename)\n",
        "            s3_path = os.path.relpath(local_path, model_dir)\n",
        "\n",
        "            s3.upload_file(\n",
        "                local_path,\n",
        "                export_bucket,\n",
        "                f\"{model_name}/{model_version}/{s3_path}\",\n",
        "                ExtraArgs={\"ACL\": \"public-read\"},\n",
        "            )\n",
        "\n",
        "    response = s3.list_objects(Bucket=export_bucket)\n",
        "    print(f\"All objects in {export_bucket}:\")\n",
        "    for file in response[\"Contents\"]:\n",
        "        print(\"{}/{}\".format(export_bucket, file[\"Key\"]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}