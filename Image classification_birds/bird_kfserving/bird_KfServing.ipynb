{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "Let's make sure Kubeflow pipeline is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: kfp\n",
      "Version: 1.0.0\n",
      "Summary: KubeFlow Pipelines SDK\n",
      "Home-page: UNKNOWN\n",
      "Author: google\n",
      "Author-email: None\n",
      "License: UNKNOWN\n",
      "Location: /usr/local/lib/python3.6/dist-packages\n",
      "Requires: Deprecated, click, requests-toolbelt, google-auth, kubernetes, jsonschema, tabulate, cloudpickle, google-cloud-storage, strip-hints, PyYAML, kfp-server-api\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "! pip show kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting minio_secret.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile minio_secret.yaml\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: minio-s3-secret\n",
    "  annotations:\n",
    "     serving.kubeflow.org/s3-endpoint: minio-service.kubeflow:9000\n",
    "     serving.kubeflow.org/s3-usehttps: \"0\" # Default: 1. Must be 0 when testing with MinIO!\n",
    "type: Opaque\n",
    "data:\n",
    "  AWS_ACCESS_KEY_ID: bWluaW8=\n",
    "  AWS_SECRET_ACCESS_KEY: bWluaW8xMjM=\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: ServiceAccount\n",
    "metadata:\n",
    "  name: default\n",
    "secrets:\n",
    "  - name: minio-s3-secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret/minio-s3-secret unchanged\n",
      "serviceaccount/default configured\n"
     ]
    }
   ],
   "source": [
    "! kubectl apply -f minio_secret.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure access MinIO\n",
    "\n",
    "Upload your Dataset to Minio\n",
    "\n",
    "First, we configure credentials for mc, the MinIO command line client. We then use it to create a bucket, upload the dataset to it, and set access policy so that the pipeline can download it from MinIO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-05-03 23:55:56--  https://dl.min.io/client/mc/release/linux-amd64/mc\n",
      "Resolving dl.min.io (dl.min.io)... 178.128.69.202\n",
      "Connecting to dl.min.io (dl.min.io)|178.128.69.202|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 20529152 (20M) [application/octet-stream]\n",
      "Saving to: ‘mc’\n",
      "\n",
      "mc                  100%[===================>]  19.58M  3.78MB/s    in 10s     \n",
      "\n",
      "2021-05-03 23:56:06 (1.95 MB/s) - ‘mc’ saved [20529152/20529152]\n",
      "\n",
      "NAME:\n",
      "  mc - MinIO Client for cloud storage and filesystems.\n",
      "\n",
      "USAGE:\n",
      "  mc [FLAGS] COMMAND [COMMAND FLAGS | -h] [ARGUMENTS...]\n",
      "\n",
      "COMMANDS:\n",
      "  alias      set, remove and list aliases in configuration file\n",
      "  ls         list buckets and objects\n",
      "  mb         make a bucket\n",
      "  rb         remove a bucket\n",
      "  cp         copy objects\n",
      "  mirror     synchronize object(s) to a remote site\n",
      "  cat        display object contents\n",
      "  head       display first 'n' lines of an object\n",
      "  pipe       stream STDIN to an object\n",
      "  share      generate URL for temporary access to an object\n",
      "  find       search for objects\n",
      "  sql        run sql queries on objects\n",
      "  stat       show object metadata\n",
      "  mv         move objects\n",
      "  tree       list buckets and objects in a tree format\n",
      "  du         summarize disk usage recursively\n",
      "  retention  set retention for object(s)\n",
      "  legalhold  manage legal hold for object(s)\n",
      "  diff       list differences in object name, size, and date between two buckets\n",
      "  rm         remove objects\n",
      "  version    manage bucket versioning\n",
      "  ilm        manage bucket lifecycle\n",
      "  encrypt    manage bucket encryption config\n",
      "  event      manage object notifications\n",
      "  watch      listen for object notification events\n",
      "  undo       undo PUT/DELETE operations\n",
      "  policy     manage anonymous access to buckets and objects\n",
      "  tag        manage tags for bucket and object(s)\n",
      "  replicate  configure server side bucket replication\n",
      "  admin      manage MinIO servers\n",
      "  update     update mc to latest release\n",
      "  \n",
      "GLOBAL FLAGS:\n",
      "  --autocompletion              install auto-completion for your shell\n",
      "  --config-dir value, -C value  path to configuration folder (default: \"/home/jovyan/.mc\")\n",
      "  --quiet, -q                   disable progress bar display\n",
      "  --no-color                    disable color theme\n",
      "  --json                        enable JSON lines formatted output\n",
      "  --debug                       enable debug output\n",
      "  --insecure                    disable SSL certificate verification\n",
      "  --help, -h                    show help\n",
      "  --version, -v                 print the version\n",
      "  \n",
      "TIP:\n",
      "  Use 'mc --autocompletion' to enable shell autocompletion\n",
      "\n",
      "VERSION:\n",
      "  RELEASE.2021-04-22T17-40-00Z\n"
     ]
    }
   ],
   "source": [
    "# download MinIO client\n",
    "\n",
    "! wget https://dl.min.io/client/mc/release/linux-amd64/mc\n",
    "! chmod +x mc\n",
    "! ./mc --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. Connect to the MinIO server**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[m\u001b[32mAdded `minio` successfully.\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! ./mc alias set minio http://minio-service.kubeflow:9000 minio minio123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**the code below clears out the bucket after downloading too many times due to testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[m\u001b[32;1mRemoving `minio/bird/datasets.tar.gz`\u001b[0m.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! ./mc rm --recursive --force minio/bird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. Create a bucket to store your data and export your model to MinIO**\n",
    "\n",
    "**Make sure you clear this bucket once you are done running your pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;3mmc: <ERROR> \u001b[0m\u001b[33;3mUnable to make bucket `minio/bird`. Your previous request to create the named bucket succeeded and you already own it.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! ./mc mb minio/bird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Upload the dataset to your bucket in MinIO.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...ts.tar.gz:  1.65 GiB / 1.65 GiB ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃ 114.84 MiB/s 14s\u001b[0m\u001b[0m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1mAccess permission for `minio/bird` is set to `download`\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! tar --dereference -czf datasets.tar.gz ./datasets\n",
    "! ./mc cp datasets.tar.gz minio/bird/datasets.tar.gz\n",
    "! ./mc policy set download minio/bird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minio Server URL and Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIO_SERVER='minio-service.kubeflow:9000'\n",
    "MINIO_ACCESS_KEY='minio'\n",
    "MINIO_SECRET_KEY='minio123'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Kubeflow Pipelines Components\n",
    "\n",
    "In this pipeline, we have the following components:\n",
    "\n",
    "* bird dataset download component\n",
    "* Train the TensorFlow model\n",
    "* Evaluate the trained model\n",
    "* Export the trained model\n",
    "* Serve model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "import kfp\n",
    "import kfp.components as components\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import InputPath, OutputPath #helps define the input & output between the components\n",
    "import kubeflow.fairing.utils\n",
    "NAMESPACE = kubeflow.fairing.utils.get_default_target_namespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 1: Download the bird Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(minio_server: str, data_dir: OutputPath(str)):\n",
    "    \"\"\"Download the bird data set to the KFP volume to share it among all steps\"\"\"\n",
    "    import urllib.request\n",
    "    import tarfile\n",
    "    import os\n",
    "    import subprocess\n",
    "\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "        \n",
    "    # this url leads to the bucket\n",
    "    url = f'http://{minio_server}/bird/datasets.tar.gz'\n",
    "    stream = urllib.request.urlopen(url)\n",
    "    tar = tarfile.open(fileobj=stream, mode=\"r|gz\")\n",
    "    tar.extractall(path=data_dir)\n",
    "    \n",
    "    subprocess.call([\"ls\", \"-lha\", data_dir])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 2: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_dir: InputPath(str), model_dir: OutputPath(str)):\n",
    "    \"\"\"Trains a ResNet101V2 for 5 epochs using a pre-downloaded dataset.\n",
    "    Once trained, the model is persisted to `model_dir`.\"\"\"\n",
    "\n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_datasets as tfds\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense,Conv2D,MaxPool2D,Flatten,Dropout,BatchNormalization,Activation\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "    \n",
    "    # import ResNet101V2 model\n",
    "    from keras.applications import ResNet101V2\n",
    "    convlayer=ResNet101V2(input_shape=(224,224,3),weights='imagenet',include_top=False)\n",
    "    convlayer.trainable = False\n",
    "    \n",
    "    # model architecture\n",
    "    model=Sequential()\n",
    "    model.add(convlayer)\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(2048,kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1024,kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(265,activation='softmax'))\n",
    "    \n",
    "    # model parameters\n",
    "    opt=tf.keras.optimizers.RMSprop(lr=0.0001)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',metrics=['accuracy'],optimizer=opt)\n",
    "    \n",
    "    # model summary\n",
    "    print(model.summary())\n",
    "    \n",
    "     \n",
    "    # load the train dataset\n",
    "    train_dir, train_info = tfds.load(\n",
    "        \"bird\",\n",
    "        split=\"train\",\n",
    "        shuffle_files=True,\n",
    "        as_supervised=True,\n",
    "        with_info=True,\n",
    "        download=True,\n",
    "        data_dir=f\"{data_dir}/datasets\",\n",
    "    )\n",
    "    \n",
    "    # generate batches of the tensor images of the training set\n",
    "    train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "    train_dir = train_datagen.flow_from_directory(train_dir,\n",
    "                                                target_size=(224,224),\n",
    "                                                classes=['ALBATROSS'],\n",
    "                                                color_mode='rgb',\n",
    "                                                class_mode='sparse',batch_size=256)\n",
    "    \n",
    "    train_dir = train_dir.cache()\n",
    "    train_dir = train_dir.shuffle(train_info.splits[\"train\"].num_examples)\n",
    "    train_dir = train_dir.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "   \n",
    "    # load the validation dataset\n",
    "    valid_dir, valid_info = tfds.load(\n",
    "        \"bird\",\n",
    "        split=\"valid\",\n",
    "        shuffle_files=True,\n",
    "        as_supervised=True,\n",
    "        with_info=True,\n",
    "        download=True,\n",
    "        data_dir=f\"{data_dir}/datasets\",\n",
    "    )\n",
    "   \n",
    "    \n",
    "    # generate batches of the tensor images of the validation set\n",
    "    val_datagen = ImageDataGenerator(rescale=1/255)\n",
    "    valid_dir = val_datagen.flow_from_directory(valid_dir,\n",
    "                                              target_size=(224,224),\n",
    "                                              classes=['ALBATROSS'],\n",
    "                                              color_mode='rgb',\n",
    "                                              class_mode='sparse',batch_size=256)\n",
    "    \n",
    "    valid_dir = valid_dir.cache()\n",
    "    valid_dir = train_dir.shuffle(valid_info.splits[\"valid\"].num_examples)\n",
    "    valid_dir = valid_dir.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "   \n",
    "    # fit the model\n",
    "    model.fit(train_dir, validation_data=valid_dir, epochs=5)\n",
    "    \n",
    "    # save the model\n",
    "    model.save(model_dir)\n",
    "    print(f\"Model saved {model_dir}\")\n",
    "    print(os.listdir(model_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 3: Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    data_dir: InputPath(str), model_dir: InputPath(str), metrics_path: OutputPath(str)\n",
    ") -> NamedTuple(\"EvaluationOutput\", [(\"mlpipeline_metrics\", \"Metrics\")]):\n",
    "    \"\"\"Loads a saved model from file and uses a pre-downloaded dataset for evaluation.\n",
    "    Model metrics are persisted to `/mlpipeline-metrics.json` for Kubeflow Pipelines\n",
    "    metadata.\"\"\"\n",
    "    \n",
    "    import json\n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_datasets as tfds\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "    from keras.preprocessing.image import load_img,img_to_array\n",
    "    from collections import namedtuple\n",
    "       \n",
    "    # load the test dataset\n",
    "    test_dir, test_info = tfds.load(\n",
    "        \"bird\",\n",
    "        split=\"test\",\n",
    "        shuffle_files=True,\n",
    "        as_supervised=True,\n",
    "        with_info=True,\n",
    "        download=True,\n",
    "        data_dir=f\"{data_dir}/datasets\",\n",
    "    )\n",
    "    \n",
    "    # generate batches of the tensor images of the test set\n",
    "    test_datagen = ImageDataGenerator(rescale=1/255)\n",
    "    test_dir = test_datagen.flow_from_directory(test_dir,\n",
    "                                                target_size=(224,224),\n",
    "                                                color_mode='rgb',\n",
    "                                                classes=['ALBATROSS'],\n",
    "                                                class_mode='sparse',batch_size=256)\n",
    "    \n",
    "    \n",
    "    test_dir = test_dir.cache()\n",
    "    test_dir = test_dir.shuffle(test_info.splits[\"test\"].num_examples)\n",
    "    test_dir = test_dir.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    # load saved model and evaluate on the test set\n",
    "    model = tf.keras.models.load_model(model_dir)\n",
    "    (loss, accuracy) = model.evaluate(test_dir)\n",
    "    \n",
    "    metrics = {\n",
    "        \"metrics\": [\n",
    "            {\"name\": \"loss\", \"numberValue\": str(loss), \"format\": \"PERCENTAGE\"},\n",
    "            {\"name\": \"accuracy\", \"numberValue\": str(accuracy), \"format\": \"PERCENTAGE\"},\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Save the metrics as a json file \n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "    out_tuple = namedtuple(\"EvaluationOutput\", [\"mlpipeline_metrics\"])\n",
    "\n",
    "    return out_tuple(json.dumps(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 4: Export the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model(\n",
    "    model_dir: InputPath(str),\n",
    "    metrics: InputPath(str),\n",
    "    export_bucket: str,\n",
    "    model_name: str,\n",
    "    model_version: int,\n",
    "    minio_server: str,\n",
    "    minio_access_key: str,\n",
    "    minio_secret_key: str,\n",
    "):\n",
    "    import os\n",
    "    import boto3\n",
    "    from botocore.client import Config\n",
    "    \n",
    "\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url=f'http://{minio_server}',\n",
    "        aws_access_key_id=minio_access_key,\n",
    "        aws_secret_access_key=minio_secret_key,\n",
    "        config=Config(signature_version=\"s3v4\"),\n",
    "    )\n",
    "\n",
    "    # Create export bucket if it does not yet exist\n",
    "    response = s3.list_buckets()\n",
    "    export_bucket_exists = False\n",
    "\n",
    "    print(response , export_bucket)\n",
    "    for bucket in response[\"Buckets\"]:\n",
    "        if bucket[\"Name\"] == export_bucket:\n",
    "            export_bucket_exists = True\n",
    "\n",
    "    if not export_bucket_exists:\n",
    "        s3.create_bucket(ACL=\"public-read-write\", Bucket=export_bucket)\n",
    "\n",
    "    # Save model files to S3\n",
    "    for root, dirs, files in os.walk(model_dir):\n",
    "        for filename in files:\n",
    "            local_path = os.path.join(root, filename)\n",
    "            s3_path = os.path.relpath(local_path, model_dir)\n",
    "\n",
    "            s3.upload_file(\n",
    "                local_path,\n",
    "                export_bucket,\n",
    "                f\"{model_name}/{model_version}/{s3_path}\",\n",
    "                ExtraArgs={\"ACL\": \"public-read\"},\n",
    "            )\n",
    "\n",
    "    response = s3.list_objects(Bucket=export_bucket)\n",
    "    print(f\"All objects in {export_bucket}:\")\n",
    "    for file in response[\"Contents\"]:\n",
    "        print(\"{}/{}\".format(export_bucket, file[\"Key\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 5: Serve Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.compiler as compiler\n",
    "import kfserving\n",
    "from kfp import components\n",
    "\n",
    "kfserving = components.load_component_from_file(\"kfserving-component.yaml\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Combine the Components into a Pipeline\n",
    "\n",
    "Up to this point we have not yet used the Kubeflow Pipelines SDK!\n",
    "\n",
    "With our four components (i.e. self-contained functions) defined, we can wire up the dependencies with Kubeflow Pipelines.\n",
    "\n",
    "The call components.func_to_container_op(f, base_image=img)(*args) has the following ingredients:\n",
    "* f is the Python function that defines a component\n",
    "* img is the base (Docker) image used to package the function\n",
    "* *args lists the arguments to f\n",
    "\n",
    "What the *args mean is best explained by going forward through the graph:\n",
    "* downloadOp is the very first step and has no dependencies; it therefore has no InputPath. Its output (i.e. OutputPath) is stored in data_dir.\n",
    "* trainOp needs the data downloaded from downloadOp and its signature lists data_dir (input) and model_dir (output). So, it depends on downloadOp.output (i.e. the previous step's output) and stores its own outputs in model_dir, which can be used by another step. downloadOp is the parent of trainOp, as required.\n",
    "* evaluateOp's function takes three arguments: data_dir (i.e. downloadOp.output), model_dir (i.e. trainOp.output), and metrics_path, which is where the function stores its evaluation metrics. That way, evaluateOp can only run after the successful completion of both downloadOp and trainOp.\n",
    "* exportOp runs the function export_model, which accepts five parameters: model_dir, metrics, export_bucket, model_name, and model_version. From where do we get the model_dir? It is nothing but trainOp.output. Similarly, metrics is evaluateOp.output. The remaining three arguments are regular Python arguments that are static for the pipeline: they do not depend on any step's output being available. Hence, they are defined without using InputPath.\n",
    "* kfservingOp is loaded from the external component and its order of execution should be specified explicitly by using kfservingOp.after(evaluateOp) function which assigns exportOp as a parent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_serve(\n",
    "    data_dir: str,\n",
    "    model_dir: str,\n",
    "    export_bucket: str,\n",
    "    model_name: str,\n",
    "    model_version: int,\n",
    "    minio_server: str,\n",
    "    minio_access_key: str,\n",
    "    minio_secret_key: str,\n",
    "):\n",
    "    # For GPU support, please add the \"-gpu\" suffix to the base image\n",
    "    BASE_IMAGE = \"mavencodev/minio:v.0.1\"\n",
    "\n",
    "    downloadOp = components.func_to_container_op(\n",
    "        download_dataset, base_image=BASE_IMAGE\n",
    "    )(minio_server)\n",
    "\n",
    "    trainOp = components.func_to_container_op(train_model, base_image=BASE_IMAGE)(\n",
    "        downloadOp.output\n",
    "    )\n",
    "\n",
    "    evaluateOp = components.func_to_container_op(evaluate_model, base_image=BASE_IMAGE)(\n",
    "        downloadOp.output, trainOp.output\n",
    "    )\n",
    "\n",
    "    exportOp = components.func_to_container_op(export_model, base_image=BASE_IMAGE)(\n",
    "        trainOp.output, evaluateOp.output, export_bucket, \n",
    "        model_name, model_version, minio_server, minio_access_key, minio_secret_key\n",
    "    )\n",
    "    \n",
    "    kfservingOp = kfserving(\n",
    "        action=\"apply\",\n",
    "        default_model_uri=f\"s3://{export_bucket}/{model_name}\",\n",
    "        model_name=\"bird\",\n",
    "        namespace= NAMESPACE,\n",
    "        framework=\"tensorflow\",\n",
    "    )\n",
    "    kfservingOp.after(exportOp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just in case it isn't obvious: this will build the Docker images. Each image is based on BASE_IMAGE and includes the Python functions as executable files. Each component can use a different base image though. This may come in handy if you want to have reusable components for automatic data and/or model analysis (e.g. to investigate bias)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have to define the pipeline itself. Our train_and_serve function defines dependencies but we must use the KFP domain-specific language (DSL) to register the pipeline with its components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def op_transformer(op):\n",
    "    op.add_pod_annotation(name=\"sidecar.istio.io/inject\", value=\"false\")\n",
    "    return op\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"End-to-End bird Pipeline\",\n",
    "    description=\"A sample pipeline to demonstrate multi-step model training, evaluation and export\",\n",
    ")\n",
    "def bird_pipeline(\n",
    "    model_dir: str = \"/train/model\",\n",
    "    data_dir: str = \"/train/data\",\n",
    "    export_bucket: str = \"bird\",\n",
    "    model_name: str = \"bird\",\n",
    "    model_version: int = 1,\n",
    "):\n",
    "    MINIO_SERVER='minio-service.kubeflow:9000'\n",
    "    MINIO_ACCESS_KEY='minio'\n",
    "    MINIO_SECRET_KEY='minio123'\n",
    "    \n",
    "    \n",
    "    train_and_serve(\n",
    "        data_dir=data_dir,\n",
    "        model_dir=model_dir,\n",
    "        export_bucket=export_bucket,\n",
    "        model_name=model_name,\n",
    "        model_version=model_version,\n",
    "        minio_server=MINIO_SERVER,\n",
    "        minio_access_key=MINIO_ACCESS_KEY,\n",
    "        minio_secret_key=MINIO_SECRET_KEY,\n",
    "    )\n",
    "    \n",
    "    dsl.get_pipeline_conf().add_op_transformer(op_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in place, let's submit the pipeline directly from our notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 210504 00:05:39 driver:124] Generating grammar tables from /usr/lib/python3.6/lib2to3/Grammar.txt\n",
      "[I 210504 00:05:39 driver:124] Generating grammar tables from /usr/lib/python3.6/lib2to3/PatternGrammar.txt\n"
     ]
    }
   ],
   "source": [
    "pipeline_func = bird_pipeline\n",
    "run_name = pipeline_func.__name__ + \" run\"\n",
    "experiment_name = \"End-to-End-Demo\"\n",
    "\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  'bird002.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
