apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: end-to-end-bird-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.0.0, pipelines.kubeflow.org/pipeline_compilation_time: '2021-05-04T00:05:39.313134',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "A sample pipeline to demonstrate
      multi-step model training, evaluation and export", "inputs": [{"default": "/train/model",
      "name": "model_dir", "optional": true, "type": "String"}, {"default": "/train/data",
      "name": "data_dir", "optional": true, "type": "String"}, {"default": "bird",
      "name": "export_bucket", "optional": true, "type": "String"}, {"default": "bird",
      "name": "model_name", "optional": true, "type": "String"}, {"default": "1",
      "name": "model_version", "optional": true, "type": "Integer"}], "name": "End-to-End
      bird Pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.0.0}
spec:
  entrypoint: end-to-end-bird-pipeline
  templates:
  - name: download-dataset
    container:
      args: [--minio-server, 'minio-service.kubeflow:9000', --data-dir, /tmp/outputs/data_dir/data]
      command:
      - python3
      - -u
      - -c
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def download_dataset(minio_server, data_dir):
            """Download the bird data set to the KFP volume to share it among all steps"""
            import urllib.request
            import tarfile
            import os
            import subprocess

            if not os.path.exists(data_dir):
                os.makedirs(data_dir)

            # this url leads to the bucket
            url = f'http://{minio_server}/bird/datasets.tar.gz'
            stream = urllib.request.urlopen(url)
            tar = tarfile.open(fileobj=stream, mode="r|gz")
            tar.extractall(path=data_dir)

            subprocess.call(["ls", "-lha", data_dir])

        import argparse
        _parser = argparse.ArgumentParser(prog='Download dataset', description='Download the bird data set to the KFP volume to share it among all steps')
        _parser.add_argument("--minio-server", dest="minio_server", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--data-dir", dest="data_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = download_dataset(**_parsed_args)
      image: mavencodev/minio:v.0.1
    outputs:
      artifacts:
      - {name: download-dataset-data_dir, path: /tmp/outputs/data_dir/data}
    metadata:
      annotations:
        sidecar.istio.io/inject: "false"
        pipelines.kubeflow.org/component_spec: '{"description": "Download the bird
          data set to the KFP volume to share it among all steps", "implementation":
          {"container": {"args": ["--minio-server", {"inputValue": "minio_server"},
          "--data-dir", {"outputPath": "data_dir"}], "command": ["python3", "-u",
          "-c", "def _make_parent_dirs_and_return_path(file_path: str):\n    import
          os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef download_dataset(minio_server, data_dir):\n    \"\"\"Download
          the bird data set to the KFP volume to share it among all steps\"\"\"\n    import
          urllib.request\n    import tarfile\n    import os\n    import subprocess\n\n    if
          not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    # this
          url leads to the bucket\n    url = f''http://{minio_server}/bird/datasets.tar.gz''\n    stream
          = urllib.request.urlopen(url)\n    tar = tarfile.open(fileobj=stream, mode=\"r|gz\")\n    tar.extractall(path=data_dir)\n\n    subprocess.call([\"ls\",
          \"-lha\", data_dir])\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Download
          dataset'', description=''Download the bird data set to the KFP volume to
          share it among all steps'')\n_parser.add_argument(\"--minio-server\", dest=\"minio_server\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--data-dir\",
          dest=\"data_dir\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = download_dataset(**_parsed_args)\n"], "image": "mavencodev/minio:v.0.1"}},
          "inputs": [{"name": "minio_server", "type": "String"}], "name": "Download
          dataset", "outputs": [{"name": "data_dir", "type": "String"}]}'
        pipelines.kubeflow.org/component_ref: '{}'
  - name: end-to-end-bird-pipeline
    inputs:
      parameters:
      - {name: export_bucket}
      - {name: model_name}
      - {name: model_version}
    dag:
      tasks:
      - {name: download-dataset, template: download-dataset}
      - name: evaluate-model
        template: evaluate-model
        dependencies: [download-dataset, train-model]
        arguments:
          artifacts:
          - {name: download-dataset-data_dir, from: '{{tasks.download-dataset.outputs.artifacts.download-dataset-data_dir}}'}
          - {name: train-model-model_dir, from: '{{tasks.train-model.outputs.artifacts.train-model-model_dir}}'}
      - name: export-model
        template: export-model
        dependencies: [evaluate-model, train-model]
        arguments:
          parameters:
          - {name: export_bucket, value: '{{inputs.parameters.export_bucket}}'}
          - {name: model_name, value: '{{inputs.parameters.model_name}}'}
          - {name: model_version, value: '{{inputs.parameters.model_version}}'}
          artifacts:
          - {name: evaluate-model-metrics, from: '{{tasks.evaluate-model.outputs.artifacts.evaluate-model-metrics}}'}
          - {name: train-model-model_dir, from: '{{tasks.train-model.outputs.artifacts.train-model-model_dir}}'}
      - name: kubeflow-serve-model
        template: kubeflow-serve-model
        dependencies: [export-model]
        arguments:
          parameters:
          - {name: export_bucket, value: '{{inputs.parameters.export_bucket}}'}
          - {name: model_name, value: '{{inputs.parameters.model_name}}'}
      - name: train-model
        template: train-model
        dependencies: [download-dataset]
        arguments:
          artifacts:
          - {name: download-dataset-data_dir, from: '{{tasks.download-dataset.outputs.artifacts.download-dataset-data_dir}}'}
  - name: evaluate-model
    container:
      args: [--data-dir, /tmp/inputs/data_dir/data, --model-dir, /tmp/inputs/model_dir/data,
        --metrics, /tmp/outputs/metrics/data, '----output-paths', /tmp/outputs/mlpipeline_metrics/data]
      command:
      - python3
      - -u
      - -c
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef evaluate_model(\n    data_dir, model_dir, metrics_path\n):\n    \"\"\
        \"Loads a saved model from file and uses a pre-downloaded dataset for evaluation.\n\
        \    Model metrics are persisted to `/mlpipeline-metrics.json` for Kubeflow\
        \ Pipelines\n    metadata.\"\"\"\n\n    import json\n    import os\n    import\
        \ tensorflow as tf\n    import tensorflow_datasets as tfds\n    from tensorflow.keras.preprocessing.image\
        \ import ImageDataGenerator\n    from keras.preprocessing.image import load_img,img_to_array\n\
        \    from collections import namedtuple\n\n    # load the test dataset\n \
        \   test_dir, test_info = tfds.load(\n        \"bird\",\n        split=\"\
        test\",\n        shuffle_files=True,\n        as_supervised=True,\n      \
        \  with_info=True,\n        download=True,\n        data_dir=f\"{data_dir}/datasets\"\
        ,\n    )\n\n    # generate batches of the tensor images of the test set\n\
        \    test_datagen = ImageDataGenerator(rescale=1/255)\n    test_dir = test_datagen.flow_from_directory(test_dir,\n\
        \                                                target_size=(224,224),\n\
        \                                                color_mode='rgb',\n     \
        \                                           classes=['ALBATROSS'],\n     \
        \                                           class_mode='sparse',batch_size=256)\n\
        \n    test_dir = test_dir.cache()\n    test_dir = test_dir.shuffle(test_info.splits[\"\
        test\"].num_examples)\n    test_dir = test_dir.prefetch(tf.data.experimental.AUTOTUNE)\n\
        \n    # load saved model and evaluate on the test set\n    model = tf.keras.models.load_model(model_dir)\n\
        \    (loss, accuracy) = model.evaluate(test_dir)\n\n    metrics = {\n    \
        \    \"metrics\": [\n            {\"name\": \"loss\", \"numberValue\": str(loss),\
        \ \"format\": \"PERCENTAGE\"},\n            {\"name\": \"accuracy\", \"numberValue\"\
        : str(accuracy), \"format\": \"PERCENTAGE\"},\n        ]\n    }\n\n    # Save\
        \ the metrics as a json file \n    with open(metrics_path, \"w\") as f:\n\
        \        json.dump(metrics, f)\n\n    out_tuple = namedtuple(\"EvaluationOutput\"\
        , [\"mlpipeline_metrics\"])\n\n    return out_tuple(json.dumps(metrics))\n\
        \nimport argparse\n_parser = argparse.ArgumentParser(prog='Evaluate model',\
        \ description='Loads a saved model from file and uses a pre-downloaded dataset\
        \ for evaluation.\\n    Model metrics are persisted to `/mlpipeline-metrics.json`\
        \ for Kubeflow Pipelines\\n    metadata.')\n_parser.add_argument(\"--data-dir\"\
        , dest=\"data_dir\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--model-dir\", dest=\"model_dir\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--metrics\", dest=\"\
        metrics_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str,\
        \ nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"\
        _output_paths\", [])\n\n_outputs = evaluate_model(**_parsed_args)\n\n_output_serializers\
        \ = [\n    str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
        \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: mavencodev/minio:v.0.1
    inputs:
      artifacts:
      - {name: download-dataset-data_dir, path: /tmp/inputs/data_dir/data}
      - {name: train-model-model_dir, path: /tmp/inputs/model_dir/data}
    outputs:
      artifacts:
      - {name: mlpipeline-metrics, path: /tmp/outputs/mlpipeline_metrics/data}
      - {name: evaluate-model-metrics, path: /tmp/outputs/metrics/data}
    metadata:
      annotations:
        sidecar.istio.io/inject: "false"
        pipelines.kubeflow.org/component_spec: '{"description": "Loads a saved model
          from file and uses a pre-downloaded dataset for evaluation.\n    Model metrics
          are persisted to `/mlpipeline-metrics.json` for Kubeflow Pipelines\n    metadata.",
          "implementation": {"container": {"args": ["--data-dir", {"inputPath": "data_dir"},
          "--model-dir", {"inputPath": "model_dir"}, "--metrics", {"outputPath": "metrics"},
          "----output-paths", {"outputPath": "mlpipeline_metrics"}], "command": ["python3",
          "-u", "-c", "def _make_parent_dirs_and_return_path(file_path: str):\n    import
          os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef evaluate_model(\n    data_dir, model_dir, metrics_path\n):\n    \"\"\"Loads
          a saved model from file and uses a pre-downloaded dataset for evaluation.\n    Model
          metrics are persisted to `/mlpipeline-metrics.json` for Kubeflow Pipelines\n    metadata.\"\"\"\n\n    import
          json\n    import os\n    import tensorflow as tf\n    import tensorflow_datasets
          as tfds\n    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n    from
          keras.preprocessing.image import load_img,img_to_array\n    from collections
          import namedtuple\n\n    # load the test dataset\n    test_dir, test_info
          = tfds.load(\n        \"bird\",\n        split=\"test\",\n        shuffle_files=True,\n        as_supervised=True,\n        with_info=True,\n        download=True,\n        data_dir=f\"{data_dir}/datasets\",\n    )\n\n    #
          generate batches of the tensor images of the test set\n    test_datagen
          = ImageDataGenerator(rescale=1/255)\n    test_dir = test_datagen.flow_from_directory(test_dir,\n                                                target_size=(224,224),\n                                                color_mode=''rgb'',\n                                                classes=[''ALBATROSS''],\n                                                class_mode=''sparse'',batch_size=256)\n\n    test_dir
          = test_dir.cache()\n    test_dir = test_dir.shuffle(test_info.splits[\"test\"].num_examples)\n    test_dir
          = test_dir.prefetch(tf.data.experimental.AUTOTUNE)\n\n    # load saved model
          and evaluate on the test set\n    model = tf.keras.models.load_model(model_dir)\n    (loss,
          accuracy) = model.evaluate(test_dir)\n\n    metrics = {\n        \"metrics\":
          [\n            {\"name\": \"loss\", \"numberValue\": str(loss), \"format\":
          \"PERCENTAGE\"},\n            {\"name\": \"accuracy\", \"numberValue\":
          str(accuracy), \"format\": \"PERCENTAGE\"},\n        ]\n    }\n\n    # Save
          the metrics as a json file \n    with open(metrics_path, \"w\") as f:\n        json.dump(metrics,
          f)\n\n    out_tuple = namedtuple(\"EvaluationOutput\", [\"mlpipeline_metrics\"])\n\n    return
          out_tuple(json.dumps(metrics))\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Evaluate
          model'', description=''Loads a saved model from file and uses a pre-downloaded
          dataset for evaluation.\\n    Model metrics are persisted to `/mlpipeline-metrics.json`
          for Kubeflow Pipelines\\n    metadata.'')\n_parser.add_argument(\"--data-dir\",
          dest=\"data_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-dir\",
          dest=\"model_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--metrics\",
          dest=\"metrics_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\", dest=\"_output_paths\",
          type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = evaluate_model(**_parsed_args)\n\n_output_serializers
          = [\n    str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "mavencodev/minio:v.0.1"}}, "inputs": [{"name": "data_dir", "type":
          "String"}, {"name": "model_dir", "type": "String"}], "name": "Evaluate model",
          "outputs": [{"name": "metrics", "type": "String"}, {"name": "mlpipeline_metrics",
          "type": "Metrics"}]}'
        pipelines.kubeflow.org/component_ref: '{}'
  - name: export-model
    container:
      args: [--model-dir, /tmp/inputs/model_dir/data, --metrics, /tmp/inputs/metrics/data,
        --export-bucket, '{{inputs.parameters.export_bucket}}', --model-name, '{{inputs.parameters.model_name}}',
        --model-version, '{{inputs.parameters.model_version}}', --minio-server, 'minio-service.kubeflow:9000',
        --minio-access-key, minio, --minio-secret-key, minio123]
      command:
      - python3
      - -u
      - -c
      - |
        def export_model(
            model_dir,
            metrics,
            export_bucket,
            model_name,
            model_version,
            minio_server,
            minio_access_key,
            minio_secret_key,
        ):
            import os
            import boto3
            from botocore.client import Config

            s3 = boto3.client(
                "s3",
                endpoint_url=f'http://{minio_server}',
                aws_access_key_id=minio_access_key,
                aws_secret_access_key=minio_secret_key,
                config=Config(signature_version="s3v4"),
            )

            # Create export bucket if it does not yet exist
            response = s3.list_buckets()
            export_bucket_exists = False

            print(response , export_bucket)
            for bucket in response["Buckets"]:
                if bucket["Name"] == export_bucket:
                    export_bucket_exists = True

            if not export_bucket_exists:
                s3.create_bucket(ACL="public-read-write", Bucket=export_bucket)

            # Save model files to S3
            for root, dirs, files in os.walk(model_dir):
                for filename in files:
                    local_path = os.path.join(root, filename)
                    s3_path = os.path.relpath(local_path, model_dir)

                    s3.upload_file(
                        local_path,
                        export_bucket,
                        f"{model_name}/{model_version}/{s3_path}",
                        ExtraArgs={"ACL": "public-read"},
                    )

            response = s3.list_objects(Bucket=export_bucket)
            print(f"All objects in {export_bucket}:")
            for file in response["Contents"]:
                print("{}/{}".format(export_bucket, file["Key"]))

        import argparse
        _parser = argparse.ArgumentParser(prog='Export model', description='')
        _parser.add_argument("--model-dir", dest="model_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--metrics", dest="metrics", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--export-bucket", dest="export_bucket", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--model-name", dest="model_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--model-version", dest="model_version", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--minio-server", dest="minio_server", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--minio-access-key", dest="minio_access_key", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--minio-secret-key", dest="minio_secret_key", type=str, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = export_model(**_parsed_args)
      image: mavencodev/minio:v.0.1
    inputs:
      parameters:
      - {name: export_bucket}
      - {name: model_name}
      - {name: model_version}
      artifacts:
      - {name: evaluate-model-metrics, path: /tmp/inputs/metrics/data}
      - {name: train-model-model_dir, path: /tmp/inputs/model_dir/data}
    metadata:
      annotations:
        sidecar.istio.io/inject: "false"
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--model-dir", {"inputPath": "model_dir"}, "--metrics", {"inputPath": "metrics"},
          "--export-bucket", {"inputValue": "export_bucket"}, "--model-name", {"inputValue":
          "model_name"}, "--model-version", {"inputValue": "model_version"}, "--minio-server",
          {"inputValue": "minio_server"}, "--minio-access-key", {"inputValue": "minio_access_key"},
          "--minio-secret-key", {"inputValue": "minio_secret_key"}], "command": ["python3",
          "-u", "-c", "def export_model(\n    model_dir,\n    metrics,\n    export_bucket,\n    model_name,\n    model_version,\n    minio_server,\n    minio_access_key,\n    minio_secret_key,\n):\n    import
          os\n    import boto3\n    from botocore.client import Config\n\n    s3 =
          boto3.client(\n        \"s3\",\n        endpoint_url=f''http://{minio_server}'',\n        aws_access_key_id=minio_access_key,\n        aws_secret_access_key=minio_secret_key,\n        config=Config(signature_version=\"s3v4\"),\n    )\n\n    #
          Create export bucket if it does not yet exist\n    response = s3.list_buckets()\n    export_bucket_exists
          = False\n\n    print(response , export_bucket)\n    for bucket in response[\"Buckets\"]:\n        if
          bucket[\"Name\"] == export_bucket:\n            export_bucket_exists = True\n\n    if
          not export_bucket_exists:\n        s3.create_bucket(ACL=\"public-read-write\",
          Bucket=export_bucket)\n\n    # Save model files to S3\n    for root, dirs,
          files in os.walk(model_dir):\n        for filename in files:\n            local_path
          = os.path.join(root, filename)\n            s3_path = os.path.relpath(local_path,
          model_dir)\n\n            s3.upload_file(\n                local_path,\n                export_bucket,\n                f\"{model_name}/{model_version}/{s3_path}\",\n                ExtraArgs={\"ACL\":
          \"public-read\"},\n            )\n\n    response = s3.list_objects(Bucket=export_bucket)\n    print(f\"All
          objects in {export_bucket}:\")\n    for file in response[\"Contents\"]:\n        print(\"{}/{}\".format(export_bucket,
          file[\"Key\"]))\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Export
          model'', description='''')\n_parser.add_argument(\"--model-dir\", dest=\"model_dir\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--metrics\",
          dest=\"metrics\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--export-bucket\",
          dest=\"export_bucket\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-name\",
          dest=\"model_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-version\",
          dest=\"model_version\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--minio-server\",
          dest=\"minio_server\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--minio-access-key\",
          dest=\"minio_access_key\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--minio-secret-key\",
          dest=\"minio_secret_key\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = export_model(**_parsed_args)\n"],
          "image": "mavencodev/minio:v.0.1"}}, "inputs": [{"name": "model_dir", "type":
          "String"}, {"name": "metrics", "type": "String"}, {"name": "export_bucket",
          "type": "String"}, {"name": "model_name", "type": "String"}, {"name": "model_version",
          "type": "Integer"}, {"name": "minio_server", "type": "String"}, {"name":
          "minio_access_key", "type": "String"}, {"name": "minio_secret_key", "type":
          "String"}], "name": "Export model"}'
        pipelines.kubeflow.org/component_ref: '{}'
  - name: kubeflow-serve-model
    container:
      args: [-u, kfservingdeployer.py, --action, apply, --model-name, bird, --default-model-uri,
        's3://{{inputs.parameters.export_bucket}}/{{inputs.parameters.model_name}}',
        --canary-model-uri, '', --canary-model-traffic, '0', --namespace, toluo, --framework,
        tensorflow, --default-custom-model-spec, '{}', --canary-custom-model-spec,
        '{}', --kfserving-endpoint, '', --autoscaling-target, '0', --service-account,
        'minio-service.kubeflow:9000', --output-path, /tmp/outputs/Service_Endpoint_URI/data]
      command: [python]
      image: aipipeline/kfserving-component:v0.3.0
    inputs:
      parameters:
      - {name: export_bucket}
      - {name: model_name}
    outputs:
      artifacts:
      - {name: kubeflow-serve-model-Service-Endpoint-URI, path: /tmp/outputs/Service_Endpoint_URI/data}
    metadata:
      annotations:
        sidecar.istio.io/inject: "false"
        pipelines.kubeflow.org/component_spec: '{"description": "Serve Model using
          Kubeflow KFServing", "implementation": {"container": {"args": ["-u", "kfservingdeployer.py",
          "--action", {"inputValue": "Action"}, "--model-name", {"inputValue": "Model
          Name"}, "--default-model-uri", {"inputValue": "Default Model URI"}, "--canary-model-uri",
          {"inputValue": "Canary Model URI"}, "--canary-model-traffic", {"inputValue":
          "Canary Model Traffic Percentage"}, "--namespace", {"inputValue": "Namespace"},
          "--framework", {"inputValue": "Framework"}, "--default-custom-model-spec",
          {"inputValue": "Default Custom Model Spec"}, "--canary-custom-model-spec",
          {"inputValue": "Canary Custom Model Spec"}, "--kfserving-endpoint", {"inputValue":
          "KFServing Endpoint"}, "--autoscaling-target", {"inputValue": "Autoscaling
          Target"}, "--service-account", {"inputValue": "Service Account"}, "--output-path",
          {"outputPath": "Service Endpoint URI"}], "command": ["python"], "image":
          "aipipeline/kfserving-component:v0.3.0"}}, "inputs": [{"default": "create",
          "description": "Action to execute on KFServing", "name": "Action", "type":
          "String"}, {"default": "bird", "description": "Name to give to the deployed
          model", "name": "Model Name", "type": "String"}, {"default": "", "description":
          "Path of the S3 or GCS compatible directory containing default model.",
          "name": "Default Model URI", "type": "String"}, {"default": "", "description":
          "Optional Path of the S3 or GCS compatible directory containing canary model.",
          "name": "Canary Model URI", "type": "String"}, {"default": "0", "description":
          "Optional Traffic to be sent to default model", "name": "Canary Model Traffic
          Percentage", "type": "String"}, {"default": "toluo", "description": "Kubernetes
          namespace where the KFServing service is deployed.", "name": "Namespace",
          "type": "String"}, {"default": "tensorflow", "description": "Machine Learning
          Framework for Model Serving.", "name": "Framework", "type": "String"}, {"default":
          "{}", "description": "Custom runtime default custom model container spec.",
          "name": "Default Custom Model Spec", "type": "String"}, {"default": "{}",
          "description": "Custom runtime canary custom model container spec.", "name":
          "Canary Custom Model Spec", "type": "String"}, {"default": "0", "description":
          "Autoscaling Target Number", "name": "Autoscaling Target", "type": "String"},
          {"default": "", "description": "KFServing remote deployer API endpoint",
          "name": "KFServing Endpoint", "type": "String"}, {"default": "minio-service.kubeflow:9000",
          "description": "Model Service Account", "name": "Service Account", "type":
          "String"}], "name": "Kubeflow - Serve Model", "outputs": [{"description":
          "URI of the deployed prediction service..", "name": "Service Endpoint URI",
          "type": "String"}]}'
        pipelines.kubeflow.org/component_ref: '{"digest": "a3161586b42ddcd28dd65e850ff993c93c80deba9a872e88dbac4c6740d287e4",
          "url": "kfserving-component.yaml"}'
  - name: train-model
    container:
      args: [--data-dir, /tmp/inputs/data_dir/data, --model-dir, /tmp/outputs/model_dir/data]
      command:
      - python3
      - -u
      - -c
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def train_model(data_dir, model_dir):
            """Trains a single-layer CNN for 5 epochs using a pre-downloaded dataset.
            Once trained, the model is persisted to `model_dir`."""

            import os
            import tensorflow as tf
            import tensorflow_datasets as tfds
            from tensorflow.keras.models import Sequential
            from tensorflow.keras.layers import Dense,Conv2D,MaxPool2D,Flatten,Dropout,BatchNormalization,Activation
            from tensorflow.keras.preprocessing.image import ImageDataGenerator

            # import ResNet101V2 model
            from keras.applications import ResNet101V2
            convlayer=ResNet101V2(input_shape=(224,224,3),weights='imagenet',include_top=False)
            convlayer.trainable = False

            # model architecture
            model=Sequential()
            model.add(convlayer)
            model.add(Dropout(0.5))
            model.add(Flatten())
            model.add(BatchNormalization())
            model.add(Dense(2048,kernel_initializer='he_uniform'))
            model.add(BatchNormalization())
            model.add(Activation('relu'))
            model.add(Dropout(0.5))
            model.add(Dense(1024,kernel_initializer='he_uniform'))
            model.add(BatchNormalization())
            model.add(Activation('relu'))
            model.add(Dropout(0.5))
            model.add(Dense(265,activation='softmax'))

            # model parameters
            opt=tf.keras.optimizers.RMSprop(lr=0.0001)
            model.compile(loss='sparse_categorical_crossentropy',metrics=['accuracy'],optimizer=opt)

            # model summary
            print(model.summary())

            # load the train dataset
            train_dir, train_info = tfds.load(
                "bird",
                split="train",
                shuffle_files=True,
                as_supervised=True,
                with_info=True,
                download=True,
                data_dir=f"{data_dir}/datasets",
            )

            # generate batches of the tensor images of the training set
            train_datagen = ImageDataGenerator(rescale=1/255)
            train_dir = train_datagen.flow_from_directory(train_dir,
                                                        target_size=(224,224),
                                                        classes=['ALBATROSS'],
                                                        color_mode='rgb',
                                                        class_mode='sparse',batch_size=256)

            train_dir = train_dir.cache()
            train_dir = train_dir.shuffle(train_info.splits["train"].num_examples)
            train_dir = train_dir.prefetch(tf.data.experimental.AUTOTUNE)

            # load the validation dataset
            valid_dir, valid_info = tfds.load(
                "bird",
                split="valid",
                shuffle_files=True,
                as_supervised=True,
                with_info=True,
                download=True,
                data_dir=f"{data_dir}/datasets",
            )

            # generate batches of the tensor images of the validation set
            val_datagen = ImageDataGenerator(rescale=1/255)
            valid_dir = val_datagen.flow_from_directory(valid_dir,
                                                      target_size=(224,224),
                                                      classes=['ALBATROSS'],
                                                      color_mode='rgb',
                                                      class_mode='sparse',batch_size=256)

            valid_dir = valid_dir.cache()
            valid_dir = train_dir.shuffle(valid_info.splits["valid"].num_examples)
            valid_dir = valid_dir.prefetch(tf.data.experimental.AUTOTUNE)

            # fit the model
            model.fit(train_dir, validation_data=valid_dir, epochs=5)

            # save the model
            model.save(model_dir)
            print(f"Model saved {model_dir}")
            print(os.listdir(model_dir))

        import argparse
        _parser = argparse.ArgumentParser(prog='Train model', description='Trains a single-layer CNN for 5 epochs using a pre-downloaded dataset.\n    Once trained, the model is persisted to `model_dir`.')
        _parser.add_argument("--data-dir", dest="data_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--model-dir", dest="model_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = train_model(**_parsed_args)
      image: mavencodev/minio:v.0.1
    inputs:
      artifacts:
      - {name: download-dataset-data_dir, path: /tmp/inputs/data_dir/data}
    outputs:
      artifacts:
      - {name: train-model-model_dir, path: /tmp/outputs/model_dir/data}
    metadata:
      annotations:
        sidecar.istio.io/inject: "false"
        pipelines.kubeflow.org/component_spec: '{"description": "Trains a single-layer
          CNN for 5 epochs using a pre-downloaded dataset.\n    Once trained, the
          model is persisted to `model_dir`.", "implementation": {"container": {"args":
          ["--data-dir", {"inputPath": "data_dir"}, "--model-dir", {"outputPath":
          "model_dir"}], "command": ["python3", "-u", "-c", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef train_model(data_dir, model_dir):\n    \"\"\"Trains a single-layer
          CNN for 5 epochs using a pre-downloaded dataset.\n    Once trained, the
          model is persisted to `model_dir`.\"\"\"\n\n    import os\n    import tensorflow
          as tf\n    import tensorflow_datasets as tfds\n    from tensorflow.keras.models
          import Sequential\n    from tensorflow.keras.layers import Dense,Conv2D,MaxPool2D,Flatten,Dropout,BatchNormalization,Activation\n    from
          tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n    #
          import ResNet101V2 model\n    from keras.applications import ResNet101V2\n    convlayer=ResNet101V2(input_shape=(224,224,3),weights=''imagenet'',include_top=False)\n    convlayer.trainable
          = False\n\n    # model architecture\n    model=Sequential()\n    model.add(convlayer)\n    model.add(Dropout(0.5))\n    model.add(Flatten())\n    model.add(BatchNormalization())\n    model.add(Dense(2048,kernel_initializer=''he_uniform''))\n    model.add(BatchNormalization())\n    model.add(Activation(''relu''))\n    model.add(Dropout(0.5))\n    model.add(Dense(1024,kernel_initializer=''he_uniform''))\n    model.add(BatchNormalization())\n    model.add(Activation(''relu''))\n    model.add(Dropout(0.5))\n    model.add(Dense(265,activation=''softmax''))\n\n    #
          model parameters\n    opt=tf.keras.optimizers.RMSprop(lr=0.0001)\n    model.compile(loss=''sparse_categorical_crossentropy'',metrics=[''accuracy''],optimizer=opt)\n\n    #
          model summary\n    print(model.summary())\n\n    # load the train dataset\n    train_dir,
          train_info = tfds.load(\n        \"bird\",\n        split=\"train\",\n        shuffle_files=True,\n        as_supervised=True,\n        with_info=True,\n        download=True,\n        data_dir=f\"{data_dir}/datasets\",\n    )\n\n    #
          generate batches of the tensor images of the training set\n    train_datagen
          = ImageDataGenerator(rescale=1/255)\n    train_dir = train_datagen.flow_from_directory(train_dir,\n                                                target_size=(224,224),\n                                                classes=[''ALBATROSS''],\n                                                color_mode=''rgb'',\n                                                class_mode=''sparse'',batch_size=256)\n\n    train_dir
          = train_dir.cache()\n    train_dir = train_dir.shuffle(train_info.splits[\"train\"].num_examples)\n    train_dir
          = train_dir.prefetch(tf.data.experimental.AUTOTUNE)\n\n    # load the validation
          dataset\n    valid_dir, valid_info = tfds.load(\n        \"bird\",\n        split=\"valid\",\n        shuffle_files=True,\n        as_supervised=True,\n        with_info=True,\n        download=True,\n        data_dir=f\"{data_dir}/datasets\",\n    )\n\n    #
          generate batches of the tensor images of the validation set\n    val_datagen
          = ImageDataGenerator(rescale=1/255)\n    valid_dir = val_datagen.flow_from_directory(valid_dir,\n                                              target_size=(224,224),\n                                              classes=[''ALBATROSS''],\n                                              color_mode=''rgb'',\n                                              class_mode=''sparse'',batch_size=256)\n\n    valid_dir
          = valid_dir.cache()\n    valid_dir = train_dir.shuffle(valid_info.splits[\"valid\"].num_examples)\n    valid_dir
          = valid_dir.prefetch(tf.data.experimental.AUTOTUNE)\n\n    # fit the model\n    model.fit(train_dir,
          validation_data=valid_dir, epochs=5)\n\n    # save the model\n    model.save(model_dir)\n    print(f\"Model
          saved {model_dir}\")\n    print(os.listdir(model_dir))\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Train model'', description=''Trains a single-layer
          CNN for 5 epochs using a pre-downloaded dataset.\\n    Once trained, the
          model is persisted to `model_dir`.'')\n_parser.add_argument(\"--data-dir\",
          dest=\"data_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-dir\",
          dest=\"model_dir\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = train_model(**_parsed_args)\n"], "image": "mavencodev/minio:v.0.1"}},
          "inputs": [{"name": "data_dir", "type": "String"}], "name": "Train model",
          "outputs": [{"name": "model_dir", "type": "String"}]}'
        pipelines.kubeflow.org/component_ref: '{}'
  arguments:
    parameters:
    - {name: model_dir, value: /train/model}
    - {name: data_dir, value: /train/data}
    - {name: export_bucket, value: bird}
    - {name: model_name, value: bird}
    - {name: model_version, value: '1'}
  serviceAccountName: pipeline-runner
