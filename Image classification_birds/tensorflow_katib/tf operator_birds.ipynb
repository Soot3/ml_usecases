{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on MNIST Dataset using Tensorflow Operator\n",
    "\n",
    "#### Prerequisites\n",
    "\n",
    "##### Before we proceed, let's check that we're using the right image, that is, TensorFlow is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.1.0 in /home/jovyan/.local/lib/python3.6/site-packages (2.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.15.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow==2.1.0) (3.15.8)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.4.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow==2.1.0) (2.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.2)\n",
      "Requirement already satisfied: gast==0.2.2 in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow==2.1.0) (0.2.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.12.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.18.5)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.8.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.0.8)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.36.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.32.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.12.1)\n",
      "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow==2.1.0) (2.1.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (2.10.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.22.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.2.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (46.1.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.23.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.4.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (7.6.3)\n",
      "Requirement already satisfied: tensorflow-datasets in /home/jovyan/.local/lib/python3.6/site-packages (4.2.0)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (6.0.7)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets) (7.16.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets) (5.0.8)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets) (5.3.4)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets) (3.5.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets) (4.3.3)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets) (1.0.0)\n",
      "Requirement already satisfied: tqdm in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow-datasets) (4.60.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow-datasets) (0.30.0)\n",
      "Requirement already satisfied: numpy in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow-datasets) (1.18.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (2.22.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (1.1.0)\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow-datasets) (5.1.2)\n",
      "Requirement already satisfied: promise in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow-datasets) (2.3)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (0.8)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (0.12.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (0.18.2)\n",
      "Requirement already satisfied: dill in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow-datasets) (0.3.3)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (3.7.4.3)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (20.3.0)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow-datasets) (3.15.8)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (1.15.0)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.4.4)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (2.7.2)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.8.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (1.4.3)\n",
      "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (2.11.2)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.1.2)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert) (4.6.3)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.5.1)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.3)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.6.0)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert) (3.2.1)\n",
      "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.17.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (2.0.10)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (46.1.3)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (4.4.2)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets) (3.2.0)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
      "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.1.5)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.52.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets) (3.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert) (1.1.1)\n",
      "Requirement already satisfied: async-generator in /usr/local/lib/python3.6/dist-packages (from nbclient<0.6.0,>=0.5.0->nbconvert) (1.10)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.6/dist-packages (from nbclient<0.6.0,>=0.5.0->nbconvert) (1.4.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert) (20.3)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert) (0.5.1)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from jedi>=0.10->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.6.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.17.3)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (1.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.0)\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (19.0.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.9.1)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.0)\n",
      "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (20.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->bleach->nbconvert) (2.4.7)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.14.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.20)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ! pip3 list | grep tensorflow \n",
    "! pip3 install --user tensorflow==2.1.0\n",
    "! pip3 install --user ipywidgets tensorflow-datasets nbconvert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To package the trainer in a container image, we shall need a file (on our cluster) that contains the code as well as a file with the resource definitition of the job for the Kubernetes cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER_FILE = \"tfjob.py\"\n",
    "KUBERNETES_FILE = \"tfjob-birds.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We also want to capture output from a cell with %%capture that usually looks like some-resource created. To that end, let's define a helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from IPython.utils.capture import CapturedIO\n",
    "\n",
    "\n",
    "def get_resource(captured_io: CapturedIO) -> str:\n",
    "    \"\"\"\n",
    "    Gets a resource name from `kubectl apply -f <configuration.yaml>`.\n",
    "\n",
    "    :param str captured_io: Output captured by using `%%capture` cell magic\n",
    "    :return: Name of the Kubernetes resource\n",
    "    :rtype: str\n",
    "    :raises Exception: if the resource could not be created\n",
    "    \"\"\"\n",
    "    out = captured_io.stdout\n",
    "    matches = re.search(r\"^(.+)\\s+created\", out)\n",
    "    if matches is not None:\n",
    "        return matches.group(1)\n",
    "    else:\n",
    "        raise Exception(f\"Cannot get resource as its creation failed: {out}. It may already exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan\n"
     ]
    }
   ],
   "source": [
    "cd/home/jovyan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Load and Inspect the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36611 images belonging to 260 classes.\n",
      "Found 1300 images belonging to 260 classes.\n",
      "Found 1300 images belonging to 260 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Conv2D,MaxPool2D,Flatten,Dropout,BatchNormalization,Activation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import load_img,img_to_array\n",
    "\n",
    "\n",
    "\n",
    "train_directory='input/train'\n",
    "val_directory='input/valid'\n",
    "test_directory='input/test'\n",
    "\n",
    "train_datagen=ImageDataGenerator(rescale=1/255)\n",
    "val_datagen=ImageDataGenerator(rescale=1/255)\n",
    "test_datagen=ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "train_generator=train_datagen.flow_from_directory(train_directory,\n",
    "                                                 target_size=(224,224),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 class_mode='sparse',batch_size=256)\n",
    "\n",
    "val_generator=val_datagen.flow_from_directory(val_directory,\n",
    "                                                 target_size=(224,224),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 class_mode='sparse',batch_size=256)\n",
    "\n",
    "test_generator=test_datagen.flow_from_directory(test_directory,\n",
    "                                                 target_size=(224,224),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 class_mode='sparse',batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer='rmsprop'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Train the Model in the Notebook\n",
    "##### We want to train the model in a distributed fashion, we put all the code in a single cell. That way we can save the file and include it in a container image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tfjob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $TRAINER_FILE\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Conv2D,MaxPool2D,Flatten,Dropout,BatchNormalization,Activation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import load_img,img_to_array\n",
    "from tensorflow.keras.applications import ResNet101V2\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_datasets_unbatched():\n",
    "    BUFFER_SIZE = 10000\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_directory='input/train'\n",
    "    val_directory='input/valid'\n",
    "    test_directory='input/test'\n",
    "\n",
    "    train_datagen=ImageDataGenerator(rescale=1/255)\n",
    "    val_datagen=ImageDataGenerator(rescale=1/255)\n",
    "    test_datagen=ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "    train_dataset=train_datagen.flow_from_directory(train_directory,\n",
    "                                                 target_size=(224,224),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 class_mode='sparse',batch_size=256)\n",
    "\n",
    "    val_dataset=val_datagen.flow_from_directory(val_directory,\n",
    "                                                 target_size=(224,224),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 class_mode='sparse',batch_size=256)\n",
    "\n",
    "    test_dataset=test_datagen.flow_from_directory(test_directory,\n",
    "                                                 target_size=(224,224),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 class_mode='sparse',batch_size=256)\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "\n",
    "def model(args):\n",
    "    convlayer=ResNet101V2(input_shape=(224,224,3),weights='imagenet',include_top=False)\n",
    "    convlayer.trainable = False\n",
    "    model=Sequential()\n",
    "    model.add(convlayer)\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(2048,kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1024,kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(260,activation='softmax'))\n",
    "    \n",
    "    opt = args.optimizer\n",
    "    \n",
    "    model.compile(optimizer=opt,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    tf.keras.backend.set_value(model.optimizer.learning_rate, args.learning_rate)\n",
    "    return model\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # MultiWorkerMirroredStrategy creates copies of all variables in the model's\n",
    "    # layers on each device across all workers\n",
    "    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(\n",
    "      communication=tf.distribute.experimental.CollectiveCommunication.AUTO)\n",
    "    logging.debug(f\"num_replicas_in_sync: {strategy.num_replicas_in_sync}\")\n",
    "    BATCH_SIZE_PER_REPLICA = args.batch_size\n",
    "    BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "\n",
    "    # Datasets need to be created after instantiation of `MultiWorkerMirroredStrategy`\n",
    "    train_dataset, val_dataset, test_dataset= make_datasets_unbatched()\n",
    "    train_dataset = train_dataset.batch(batch_size=BATCH_SIZE)\n",
    "    val_dataset = val_dataset.batch(batch_size=BATCH_SIZE)\n",
    "    test_dataset = test_dataset.batch(batch_size=BATCH_SIZE)\n",
    "\n",
    "    # See: https://www.tensorflow.org/api_docs/python/tf/data/experimental/DistributeOptions\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = \\\n",
    "        tf.data.experimental.AutoShardPolicy.DATA\n",
    "\n",
    "    train_datasets_sharded  = train_dataset.with_options(options)\n",
    "    val_datasets_sharded  = val_dataset.with_options(options)\n",
    "    test_dataset_sharded = test_dataset.with_options(options)\n",
    "\n",
    "    with strategy.scope():\n",
    "        # Model building/compiling need to be within `strategy.scope()`.\n",
    "        multi_worker_model = model(args)\n",
    "\n",
    "    # Keras' `model.fit()` trains the model with specified number of epochs and\n",
    "    # number of steps per epoch. \n",
    "    multi_worker_model.fit(train_datasets_sharded,validation_data=val_datasets_sharded,\n",
    "                         epochs=5,\n",
    "                         steps_per_epoch=10)\n",
    "\n",
    "  \n",
    "    eval_loss, eval_acc = multi_worker_model.evaluate(test_dataset_sharded, \n",
    "                                                    verbose=0, steps=10)\n",
    "\n",
    "\n",
    "    # Log metrics for Katib\n",
    "    logging.info(\"loss={:.4f}\".format(eval_loss))\n",
    "    logging.info(\"accuracy={:.4f}\".format(eval_acc))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--batch_size\",\n",
    "                      type=int,\n",
    "                      default=128,\n",
    "                      metavar=\"N\",\n",
    "                      help=\"Batch size for training (default: 128)\")\n",
    "    parser.add_argument(\"--learning_rate\", \n",
    "                      type=float,  \n",
    "                      default=0.001,\n",
    "                      metavar=\"N\",\n",
    "                      help='Initial learning rate')\n",
    "    parser.add_argument(\"--optimizer\", \n",
    "                      type=str, \n",
    "                      default='adam',\n",
    "                      metavar=\"N\",\n",
    "                      help='optimizer')\n",
    "\n",
    "    parsed_args, _ = parser.parse_known_args()\n",
    "    main(parsed_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### That saves the file as defined by TRAINER_FILE but it does not run it.\n",
    "\n",
    "##### Let's see if our code is correct by running it from within our notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run $TRAINER_FILE --optimizer $optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Docker Image Manually\n",
    "##### The Dockerfile looks as follows:\n",
    "\n",
    "FROM tensorflow/tensorflow:2.4.0\n",
    "RUN pip install tensorflow_datasets\n",
    "COPY tfjob.py /\n",
    "ENTRYPOINT [\"python\", \"/tfjob.py\", \"--batch_size\", \"100\", \"--learning_rate\", \"0.001\", \"--optimizer\", \"adam\"]\n",
    "If GPU support is not needed, you can leave off the -gpu suffix from the image. mnist.py is the trainer code you have to download to your local machine.\n",
    "\n",
    "Then it's easy to push images to your container registry:\n",
    "\n",
    "docker build -t <docker_image_name_with_tag> .\n",
    "docker push <docker_image_name_with_tag>\n",
    "The image is available as mavencodev/tf_job:5.0 in case you want to skip it for now.\n",
    "\n",
    "### How to Create a Distributed TFJob\n",
    "##### For large training jobs, we wish to run our trainer in a distributed mode. Once the notebook server cluster can access the Docker image from the registry, we can launch a distributed PyTorch job.\n",
    "\n",
    "##### The specification for a distributed TFJob is defined using YAML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $KUBERNETES_FILE\n",
    "apiVersion: \"kubeflow.org/v1\"\n",
    "kind: \"TFJob\"\n",
    "metadata:\n",
    "  name: \"birdsjob\"\n",
    "  namespace: ekemini # your-user-namespace\n",
    "spec:\n",
    "  cleanPodPolicy: None\n",
    "  tfReplicaSpecs:\n",
    "    Worker:\n",
    "      replicas: 2\n",
    "      restartPolicy: OnFailure\n",
    "      template:\n",
    "        metadata:\n",
    "          annotations:\n",
    "            sidecar.istio.io/inject: \"false\"\n",
    "        spec:\n",
    "          containers:\n",
    "          - name: tensorflow\n",
    "            # modify this property if you would like to use a custom image\n",
    "            image: mavencodev/tfbirds:v.0.3\n",
    "            command:\n",
    "                - \"python\"\n",
    "                - \"/tfjob.py\"\n",
    "                - \"--batch_size=150\"\n",
    "                - \"--learning_rate=0.001\"\n",
    "                - \"--optimizer=adam\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's deploy the distributed training job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture tf_output --no-stderr\n",
    "! kubectl create -f $KUBERNETES_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_JOB = get_resource(tf_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To see the job status, use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl describe $TF_JOB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### You should now be able to see the created pods matching the specified number of workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl get pods -l job-name=birdsjob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In case of issues, it may be helpful to see the last ten events within the cluster:\n",
    "\n",
    "#####                                         ! kubectl get events --sort-by='.lastTimestamp' | tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl get events --sort-by='.lastTimestamp' | tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To stream logs from the worker-0 pod to check the training progress, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl logs -f birds-worker-0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To delete the job, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! kubectl delete tfjob --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check to see if the check to see if the pod is still up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! kubectl -n ekemini logs -f mnist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
